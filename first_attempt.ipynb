{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **A/B Tests**\n",
    "\n",
    "Imagine you are responsible for a service, say a website for pet adoption. You care about people booking a meeting to come visit and possibly adopt a stray. You have social-media outreach and key-word advertising to drive traffic, and you care specifically about the **conversion rate**: among the visitors who land on the home page, how many book a meeting.\n",
    "\n",
    "To improve that conversion rate, you have an idea for an improvement: a cleaner page design or a more compelling call-to-action. You are not sure how effective it will be. The best way to do that is to **split** customers in two and show one half your existing set-up ('Control' or A) and the other half your modified, hopefully improved version ('Treatement', or B). If the improved version leads to a clearly better conversion, you probably want to adopt it.\n",
    "\n",
    "It compares the two using a $t$-test to decide whether the observed difference is large enough to be meaningful, what we call **statistically significant**.\n",
    "\n",
    "Spliting the traffic to an on-line service and comparing conversion rates with a $t$-test is a univerally used approach to understand the impact of any change. In science, itâ€™s known as a **random control trial** (RCT) but online services prefer to call it an **A/B-test**.\n",
    "\n",
    "\n",
    "In A/B testing, two versions of the same marketing material are created: version A (the control group) and version B (the treatment group). The two versions are then randomly shown to different groups of users, and their responses are measured and compared.\n",
    "\n",
    "In A/B testing, the treatment group is the group that receives the modified version of the marketing material being tested (version B), while the control group is the group that receives the original or existing version of the marketing material (version A).\n",
    "\n",
    "The purpose of the control group is to establish a baseline or benchmark against which the performance of the treatment group can be measured. By measuring the performance of both groups, analysts can determine whether the changes made in the treatment group had a statistically significant impact on the measured response metrics.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **T-Test**\n",
    "\n",
    "A t-test is a statistical test used to determine if there is a significant difference between the response metrics of the treatment group and the control group. It helps in evaluating whether the observed difference in performance between the two groups is statistically significant or simply due to random chance.\n",
    "\n",
    "There are two common types of t-tests used in A/B testing: the independent samples t-test and the paired samples t-test.\n",
    "\n",
    "Independent samples t-test: This type of t-test is used when the treatment and control groups are independent of each other, meaning that each participant is assigned to either the treatment or control group, but not both. The independent samples t-test compares the means of the response metrics between the two groups. It assumes that the response metrics are normally distributed and have equal variances.\n",
    "\n",
    "Paired samples t-test: This type of t-test is used when the treatment and control groups are dependent on each other. It is employed when each participant is exposed to both the treatment and control conditions, such as in a before-and-after scenario. The paired samples t-test compares the mean differences between the response metrics of the two conditions. It also assumes that the differences are normally distributed.\n",
    "\n",
    "In both types of t-tests, the null hypothesis assumes that there is no significant difference between the response metrics of the treatment and control groups. The alternative hypothesis suggests that there is a significant difference. By calculating the t-statistic and comparing it to the critical value from the t-distribution, a p-value is obtained. If the p-value is below a predetermined significance level (typically 0.05), the null hypothesis is rejected, indicating a statistically significant difference between the groups.\n",
    "\n",
    "T-tests provide a statistical framework to assess the significance of the observed differences in response metrics between the treatment and control groups, helping to determine the effectiveness of the changes made in the A/B test."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import \n",
    "## Key libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event log data from a pickle file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = pd.read_pickle('../data/events.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the datatype\n",
    "events_df['event_timestamp'] =\\\n",
    "    events_df['event_timestamp'].astype('datetime64[ns]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_conversion = events_df\\\n",
    "    .groupby(['page_url_path', events_df.event_timestamp.dt.to_period('D')])\\\n",
    "    .agg({'user_domain_id': 'count'})\\\n",
    "    .unstack(level='page_url_path')\n",
    "daily_conversion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_conversion.columns = daily_conversion.columns.droplevel()\n",
    "order=['/home','/product_a','/product_b','/cart','/payment','/confirmation']\n",
    "daily_conversion = daily_conversion[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_conversion.plot()\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data processsing \n",
    "## Aggregate into conversion rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_users = events_df\\\n",
    "    .groupby(['variant'])\\\n",
    "    ['user_domain_id'].nunique()\n",
    "conversion_count = events_df\\\n",
    "    .groupby(['variant', 'page_url_path'])\\\n",
    "    ['user_domain_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_rate = pd.DataFrame(conversion_count)\\\n",
    "    .join(total_users, on='variant', rsuffix='_total')\n",
    "conversion_rate['conversion_rate'] =       \\\n",
    "    conversion_rate['user_domain_id'] /    \\\n",
    "    conversion_rate['user_domain_id_total']\n",
    "conversion_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_rate.drop(\n",
    "    columns=['user_domain_id'], inplace=True)\n",
    "conversion_rate.columns = \\\n",
    "    ['visitors', 'conversion_rate']\n",
    "conversion_rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot into parallel columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_rate = conversion_rate\\\n",
    "    .pivot_table(\n",
    "        index='page_url_path',\n",
    "        columns='variant',\n",
    "        values=['visitors', 'conversion_rate']) \\\n",
    "    .sort_values(\n",
    "        ('conversion_rate', 'Treatment'),\n",
    "        ascending=False\n",
    "    )\n",
    "conversion_rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compute statistical tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $t$-test\n",
    "\n",
    "The most commonly used statistical test is the $t$-test.\n",
    "\n",
    "$t$ is the **normalised** difference between two measures using the expected standard deviation:\n",
    "\n",
    "$$t = \\frac{\\Delta\\overline{X}}{s_{\\Delta\\bar{X}}}$$\n",
    "\n",
    "where \n",
    "* $X$ is our objective (conversion),\n",
    "*  $\\overline{X}$ the average over a sample (conversion rate)\n",
    "*  $\\Delta\\overline{X}$ the difference, and \n",
    "*  $s_{\\Delta\\overline{X}}$ is the **standard deviation** of that difference, i.e. the commonly observed difference between two indepedent measurements from the same origin.\n",
    "\n",
    "This score can be compared to a normalised $t$-distribution:\n",
    "\n",
    "![t-distribution](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c0/Student_T-Distribution_Table_Diagram.svg/2560px-Student_T-Distribution_Table_Diagram.svg.png)\n",
    "\n",
    "If we assumed there were no difference, when $t$ is very large (typically around 2), that observersation is an outlier. Therefore, we would have to reject that assumption. We consider that difference significant.\n",
    "\n",
    "## Welchâ€™s $t$-test\n",
    "\n",
    "Welch suggested a version when both Control and Treatment offer an estimate the standard deviation: \n",
    "$$ t = \\frac{\\overline{X}_T - \\overline{X}_C}\n",
    "            {\\sqrt{\n",
    "               {s_{\\overline{X}_T}^2} +\n",
    "               {s_{\\overline{X}_C}^2}\n",
    "            }} $$\n",
    "\n",
    "With conversion rate, the variance of one observation can be estimated as:\n",
    "\n",
    "$$ s_{X_Ti}^2 = \\overline{X}_T (1-\\overline{X}_T) \\qquad\n",
    "   s_{X_Ci}^2 = \\overline{X}_C (1-\\overline{X}_C)$$\n",
    "\n",
    "The variance of the sum of $n$ independent observations of variance $s^2$ is $n$ times larger. The variance of an average, $n^2$ smaller:\n",
    "$$ s_{\\overline{X}_T}^2 = \\frac{n.s^2}{n^2} = \\frac{s_{X_Ti}^2}{n} \\qquad\n",
    "   s_{\\overline{X}_C}^2 = \\frac{s_{X_Ci}^2}{n}\n",
    "$$\n",
    "\n",
    "Therefore, \n",
    "$$ t = \\frac{\\overline{X}_T - \\overline{X}_C}\n",
    "            {s_{\\Delta\\overline{X}}} $$\n",
    "where \n",
    "$$ s_{\\Delta\\overline{X}} = \\sqrt{\n",
    "   \\frac{\\overline{X}_T(1-\\overline{X}_T)}{n_T} +\n",
    "   \\frac{\\overline{X}_C(1-\\overline{X}_C)}{n_C}\n",
    "} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "alpha = 0.05\n",
    "\n",
    "def compute_t_test(c):    \n",
    "    c['difference'] =                               \\\n",
    "        c['conversion_rate']['Treatment'] -         \\\n",
    "        c['conversion_rate']['Control']\n",
    "    c['stdev'] = (\n",
    "            c['conversion_rate']['Treatment'] *     \\\n",
    "            (1 - c['conversion_rate']['Treatment'])/\n",
    "                c['visitors']['Treatment'] +  \n",
    "            c['conversion_rate']['Control'] *       \\\n",
    "            (1 - c['conversion_rate']['Control'])/\n",
    "                c['visitors']['Control']\n",
    "        ) ** 0.5\n",
    "    c['t-score'] = c['difference'] / c['stdev']\n",
    "    c['degrees_freedom'] =                          \\\n",
    "        c['visitors']['Control'] +                  \\\n",
    "        c['visitors']['Treatment'] - 1\n",
    "    c['p-value'] = t.sf(\n",
    "        np.abs(c['t-score']),\n",
    "        c['degrees_freedom']\n",
    "    ) * 2\n",
    "    c['minimum_detectable_effect'] =                \\\n",
    "        t.ppf(1 - alpha/2, c['degrees_freedom']) *  \\\n",
    "        c['stdev']\n",
    "    c['significant'] = c['p-value'] < alpha\n",
    " \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [alpha/2, 1 - alpha/2]:\n",
    "    print(f'{d=}: t={t.ppf(d, 1000)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of the t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_t_test(conversion_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Questions\n",
    "\n",
    "* What is A/B testing, and why is it commonly used in data-driven decision-making?\n",
    "\n",
    "* What is the difference between the treatment group and the control group in an A/B test?\n",
    "\n",
    "* How do you determine the sample size for an A/B test? What factors influence this decision?\n",
    "\n",
    "\n",
    "* What is hypothesis testing, and how does it apply to A/B testing?\n",
    "\n",
    "* What is the p-value in A/B testing, and how do you interpret its significance?\n",
    "\n",
    "* What are Type I and Type II errors in A/B testing, and how do they relate to statistical significance and statistical power?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
